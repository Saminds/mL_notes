# ğŸ§  Session 1: Introduction to Machine Learning

## âœ… The Learning Problem

A machine learning problem is defined by:

- **T**: Task  
- **P**: Performance Measure  
- **E**: Experience  

This is often written as:  
**Learning Problem = (T, P, E)**

The goal is to use experience (data) to improve performance on a given task for unseen inputs.

---

## ğŸ“ˆ Regression and Supervised Learning

- A **regression problem** is one where we predict a **continuous value** from features.  
- **Linear regression** means we fit a line to our data.  

**Supervised learning** includes:
- **Regression algorithms**: predict continuous targets  
- **Classification algorithms**: predict discrete class labels  

Other learning paradigms:
- **Semi-supervised learning**  
- **Active learning**  
- **Online learning**

---

## ğŸ” Supervised Learning as Function Approximation

In supervised learning, the model learns a function to predict output `y` given input `x`.

The goal is to estimate a function:  

f: â„^D â†’ â„, such that y â‰ˆ f(x) + Îµ

Where:
- `x`: input feature vector
- `y`: target output
- `Îµ`: random noise

---

## ğŸ“ Hypothesis Space

We define a **hypothesis set** as:
H = { h(x, Î¸) | Î¸ âˆˆ Î˜ }
Where:
- `h(x, Î¸)` is a candidate function
- `Î¸` represents learnable parameters

A **learning algorithm** selects the best hypothesis `h(x, Î¸*)` by finding:

Î¸* = argmin J(Î¸)

---

## ğŸ”¢ Vector Representation

- Input vector:  
  `x = [xâ‚€, xâ‚, ..., x_D]`

- Parameter vector (weights):  
  `Î¸ = [Î¸â‚€, Î¸â‚, ..., Î¸_D]`

A **linear model** (hypothesis function) is defined as:  
h_Î¸(x) = Î¸â‚€ + Î¸â‚Â·xâ‚ + Î¸â‚‚Â·xâ‚‚ + ... + Î¸_DÂ·x_D
or in vector form:  
h_Î¸(x) = Î¸áµ€Â·x

---

## ğŸ¯ Cost Function & Optimization

We define a **cost function** to measure prediction error.

For a single data point:  
SE = (yáµ¢ - h_Î¸(xáµ¢))Â²

For the whole dataset (Sum of Squared Errors):  
J(Î¸) = Î£ (yáµ¢ - Î¸áµ€Â·xáµ¢)Â²

**Objective**: Find the best `Î¸` that minimizes the cost:  
Î¸* = argmin J(Î¸)
